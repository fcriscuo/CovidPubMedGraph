#'
#' Script name: top_n_articles.R
#'
#' Purpose of script: This R script is responsible for determining the 
#' most cited articles from an inputted list of PubMed Ids. The primary
#' use case is to identify the most important articles, based on the cited-by
#' criterion from a large population pf PubMed articles. It is anticipated that
#' the number of selected articles will only be a small percentage (e.g. 0.1%) 
#' of the original input
#' 
#' This script utilizes the eutils Web service provided by NCBI to determine
#' how many times a specified article has been cited by other articles.
#' NCBI requests that non-registered users submit a maximum of 3 queries per
#' second; registered users may submit up to 10 queries per second. This script
#' incorporates a 0.1 second delay to meet the latter requirement
#' 
#' In order to accommodate interruptions in submitting queries to NCBI, this 
#' script is designed to be restartable. Interim results are written to a 
#' snapshot file on a periodic basis. On startup, this script will use that
#' file as a starting point for subsequent processing. If that file does not
#' exist, the script will start at the first Pubmed Id in the input file.
#' 
#' Caveat: PubMed is a dynamic repository of scientific literature information.
#' The results generated by this script will not be reproducible since PubMed 
#' contents are constantly expanding.
#'
#' Author:Fred Criscuolo
#'
#' Date Created: 2021-06-03
#'
#' Copyright (c) Fred Criscuolo, 2021
#' Email: genomicdatasci@gmail.com
#'
#' ---------------------------
#'
#' Notes: 1. The eutils URL to query NCBI will be composed using an API key
#'           and email address specified in the user's .Renviron file.
#'        2. It is assumed that the input file will have column names in the
#'           first record. This record is ignored in order for the script to 
#'           be restartable.
#'   
#'
#' ---------------------------

#' Pacman package
if (!require("pacman")) install.packages("pacman"); library(pacman)
pacman::p_load("tidyverse", "data.table",  "httr","XML","curl",
               "digest","properties","magrittr", "readr",
               "logger", "rentrez","fs")

source(here::here("R/utilities/renviron_properties.R"))

log_appender(appender_file(here::here("./logs/top_cited_articles.log")))

#' Redefine input file column names as required 
pubmed_column_names =c("cord_uid","sha","source_x","title","doi","pmcid","pubmed_id",
                       "license","abstract","publish_time","authors","journal",
                       "mag_id","who_covidence_id","arxiv_id","pdf_json_files",
                       "pmc_json_files","url","s2_id")

#' Function will extract a list PubMed Ids from an input file
#' Relevant column name is assummed to be pubmed_id
#' 
extract_pubmed_ids_from_csv <- function(csv_file_path, row_count = Inf, skip_rows = 1) {
  # Accept function defaults
  # Include col_names = TRUE (default) to document header requirement
  pubmed_id_list <- read_csv(csv_file_path,col_names = pubmed_column_names,
                             guess_max = 4,
                             n_max = row_count, skip = skip_rows) %>% 
    filter(!is.na(pubmed_id)) %>% 
    select(pubmed_id)
  log_info(paste("Read ", nrow(pubmed_id_list), " from csv file: ", csv_file_path, sep =""))
  log_info(paste("Skipped ", skip_rows," rows", sep=""))
  return (pubmed_id_list)
}

#' Function that will read the specified release of the Covid-19 metadata.csv 
#' file. If necessary the function will create a release-specific subdirectory
#' for this file.
fetch_covid19_metadata <- function(release = "2021-06-14") {
  metadata_file <- here::here(paste("protected_data/",release,"/metadata.csv",sep=""))
  if (file_exists(metadata_file)) {
    log_info(paste(metadata_file, " already exists", sep=""))
    return(metadata_file)
  }
  base_url <- "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/RELEASE/metadata.csv"
  dest_dir <- here::here(paste("protected_data",release, sep="/"))
  dir_create(dest_dir)
  url <- stringr::str_replace(base_url,"RELEASE", release)
  download.file(url,metadata_file ,"curl")
  log_info(paste("COVID-19 metadata loaded to : ", url.sep=""))
  return (paste(dest_dir,"metadata.csv",sep="/"))
}

#'Function that submits an eutils query to NCBI requesting the PubMed Ids for
#'articles that cite the specified PubMed Id. The hypothesis is that the 
#'importance of an article is proportional to how often it has been used as
#'a reference.
#' TODO: refactor this function to use the xml2 package in lieu of the XML package
#' 
fetch_cited_by_pubmed_ids <- function(pubmed_id) {
  base_url <- paste(
    "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&linkname=pubmed_pubmed_citedin&id=PUBMED_ID&&tool=my_tool&email=",ncbi_email,"&api_key=",ncbi_api_key,sep = "")
  df <- tibble(cite_id = character())
  url <-stringr::str_replace(base_url, 'PUBMED_ID', as.character(pubmed_id))
  UA <-
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36"
   tryCatch({
    doc <- GET(url, user_agent(UA))
    data <- XML::xmlParse(content(doc, "text"))
    nodes <- getNodeSet(data, '//LinkSetDb')
    if (!is.null(nodes) && length(nodes) > 0) {
      links <- xmlChildren(nodes[[1]])
      for (i in 1:length(links)) {
        link <- xmlChildren(links[[i]])$Id
        novel_id <-   xmlValue(link)
        if (!is.na(novel_id)) {
          df[nrow(df) + 1, ] <- novel_id
        }
      }
      return (df)
    }
  } ,
  error = function(cond) {
    message(paste("ERROR getting URL ", url, " PubMed id: ", pubmed_id, sep =
                    ""))
    message(cond)
    log_info(paste("Error processing PubMed Id: ", pubmed_id, sep = ""))
    return (df)
  },
  warning = function(cond) {
    message(paste("Warning getting URL ", url, "PubMed id: ", pubmed_id, sep =
                    ""))
    message(cond)
    return (df)
  },
  finally = {
  })
  return (df)
}

#'Function that maintains the tibble listing the top n cited articles.
#'If the latest article has a cited-by count that is higher than the current 
#'data frame entry with the lowest cited-by count, the data frame entry
#'is replaced. 
#'The sequence number of the latest entry in the source file is also 
#'persisted to provide a starting point for resuming processing after
#'an interruption
#'
update_top_articles <- function(top_articles, pm_id, cited_by_count,num) {
  # identify the current article with the lowest cited by count
  x <- top_articles[which.min(top_articles$count),]
  if (x$count[1] < cited_by_count) {
    index <- x$id[1]
    updated_top_articles <- rows_update(top_articles, tibble(id=index,
                                                             pubmed_id=as.character(pm_id),
                                                             count= cited_by_count,
                                                             seq_num = num),
                                        by = "id")
    return (updated_top_articles)
  }
  return (top_articles)
}


#'Function that initiates a tibble listing the top n cited articles.
#'If a snapshot file exists, it is used to initialize the tibble values.
#'The maximum sequence number in that file is used to determine how many
#'rows in the input file have already been processed
#'If a snapshot file does not exist, the tibble is initiated with
#'dummy values
#'
init_top_articles_data_frame <- function(snapshot_file_path, n) {
  if (!file.exists(snapshot_file_path)) {
    id <- seq(1:n)
    pubmed_id <- rep("XXXX", times = n)
    count <-  rep(0, times = n)
    seq_num <- rep(0, times = n)
    log_info("Created new tibble for top cited articles")
    return(tibble(id, pubmed_id, count, seq_num))
  }
  # Initialize data frame from last snapshot
  log_info("Reading snapshot file")
  return(read_csv(
    snapshot_file_path,
    col_names = TRUE,
    col_types = list(
      "id" = col_integer(),
      "pubmed_id" = col_character(),
      "count" = col_integer(),
      "seq_num" = col_integer()
    )
  ))
}

#'Function to determine which PubMed entry to start with
#'Supports restarting the script after an incomplete scan
#'This number represents the last PubMed entry that qualified for
#'inclusion is the tibble and may be lower than the actual 
#'record processed befor the previous interruption
#'
determine_starting_seq_num <- function(top_articles){
  seq_num <-  max(as.integer(top_articles$seq_num),na.rm = TRUE ) +1
  log_info(paste("The starting pubmed sequence number is ", seq_num, sep = ""))
  return(seq_num)
}

# Function to maintain a list of the top n cited PubMed entries
# input is the number of entries to maintain
select_top_n_cited_articles <-
  function(csv_file_name = "protected_data/metadata_sample.csv"
           , n ) {
    csv_file <- here::here(csv_file_name)
    snapshot_file <-  here::here("tmp/top_cited_by_snapshot.csv")
    out_file_path <-
      here::here(paste("data/top_", n, "_articles.csv", sep = ""))
    top_articles <- init_top_articles_data_frame(snapshot_file, n)
    seq_num <-  determine_starting_seq_num(top_articles) 
    pubmed_ids <- extract_pubmed_ids_from_csv(csv_file, Inf,seq_num)
    for (i in 1:nrow(pubmed_ids)) {
        seq_num <- seq_num +1
      # determine the cited by count for this article
      pubmed_id <-  as.character(pubmed_ids$pubmed_id[i])
      df <-  fetch_cited_by_pubmed_ids((pubmed_id))
      if (!is.null(nrow(df)) & nrow(df) > 0) {
        top_articles <-
          update_top_articles(top_articles, pubmed_id, nrow(df), seq_num)
      }
      # save interim results
      if (i %% 100 == 0) {
        top_articles %>% 
          arrange(desc(count))  %>% 
        write_csv(., snapshot_file)
        log_info(paste("Snapshot file wirtten at input row: ", seq_num, sep = ""))
      }
      Sys.sleep(0.1)  # limit rate of requests sent to NCBI
    }
    # Output final results to csv file
    cited_by <- top_articles %>%
      arrange(desc(count))
    write_csv(cited_by, out_file_path)
  }
